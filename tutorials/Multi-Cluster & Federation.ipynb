{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1beffcb-8a59-4f27-84e3-d4a3908a9511",
   "metadata": {},
   "source": [
    "# Multi-Cluster & Federation\n",
    "Multi-cluster operation and federation are advance features of Slurm. When multiple clusters are connected to the same slurmdb account services, Multi-cluster operation allows job submission, monitoring, configure changes across clusters from clients of any one of the cluster. On top of that, when federation is configured, jobs submitted to a cluster member can be replicated to all/subset of other cluster in the federation as sibling jobs, and having a unified jobid space to uniqly identify a job globally. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60116f27-791b-42c2-bc32-70ea5f329874",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "The architecture of this container lab is expanded for trying out the multi-cluster & federation feature. \n",
    "![multi-cluster-and-federation](./multi-cluster-and-federation.png)\n",
    "A new cluster \"lyoko\" is added to the architecture, and will use the existing slurmdbd services hosted on container slurm-lab-master-[1-2]. You Can do this by uncommenting the corresponding objects in ./compose.dev.yml and update the cluster.\n",
    "```\n",
    "podman compose -f compose.dev.yml up -d \n",
    "```\n",
    "At this point, multi-cluster operation is enabled automatically as they both use the same slurmdbd service. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57b51ff-3ab6-4db6-bc52-a77088d2263d",
   "metadata": {},
   "source": [
    "## Multi-Cluster Operation\n",
    "Although this client container is only a part of slurm-lab cluster, slurm command can be used to query / control / interact with the other cluster (lyoko) that uses the same slurmdbd service with option `-M`, or `--clusters`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8ba63-a464-490a-88a0-f9f9363a4df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sinfo --long --clusters slurm-lab,lyoko\n",
    "sinfo --N --long --clusters slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdae471-2b9b-4265-bd0f-c20f2a0d01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "squeue -la --clusters slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c54f20-3a3b-4579-8331-aa7496b5f707",
   "metadata": {},
   "source": [
    "### Job submission\n",
    "For job submission you list one or more cluster, one job will be submitted to the cluster that is estimated to start the job the earliest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab630c-71ed-42c5-b538-457d6481a4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit one job to slurm-lab, the local cluster\n",
    "jobid_0=$( sbatch --ntasks=2 --parsable --time 00:10:00 endless-checksum-mpi.sh )\n",
    "\n",
    "# submit one job to lyoko, the remote cluster\n",
    "# parsable output format: <jobid>;<cluster>\n",
    "IFS=';' read -r jobid_1 cluster_1 < <( sbatch --cluster lyoko --ntasks=2 --parsable --time 00:10:00 endless-checksum-mpi.sh )\n",
    "\n",
    "# submit one job to multi cluster, see where it lands\n",
    "# parsable output format: <jobid>;<cluster>\n",
    "IFS=';' read -r jobid_2 cluster_2 < <( sbatch --cluster slurm-lab,lyoko --ntasks=2 --parsable --time 00:10:00 endless-checksum-mpi.sh )\n",
    "\n",
    "cat <<EOF\n",
    "Job ${jobid_0} submitted to local cluster (slurm-lab)\n",
    "Job ${jobid_1} submitted to remote cluster (${cluster_1})\n",
    "Job ${jobid_2} submitted to earliest available cluster ${cluster_2}\n",
    "EOF\n",
    "\n",
    "squeue -la --clusters slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f9719-de8c-4a74-8da9-cdaf18e84389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cancel local job\n",
    "scancel ${jobid_0}\n",
    "\n",
    "# cancel job on specific cluster\n",
    "scancel --clusters ${cluster_1} ${jobid_1}\n",
    "scancel --clusters ${cluster_2} ${jobid_2}\n",
    "\n",
    "# check queue\n",
    "squeue -la --clusters slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9a4bc-6fb5-4f54-aef1-c90a5e4f6b9f",
   "metadata": {},
   "source": [
    "IMPORTANT: If multiple cluster is given, scacel will cancel jobs of specified job id on ALL of them. Job ID are NOT unique!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9acfda0-2c1e-46de-9c88-29d087de7c33",
   "metadata": {},
   "source": [
    "### scontrol\n",
    "For `scontrol` it is a little bit different, ONLY ONE cluster can be specified in the option, fatal error otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcca80f-f27f-47b3-9bbd-d2e3a6b738b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example of fatal error\n",
    "scontrol --clusters slurm-lab,lyoko update nodename=ALL state=drain reason=\"this cmd is going to fail\"\n",
    "\n",
    "# draing nodes in the remote cluster \n",
    "scontrol --cluster lyoko update nodename=ALL state=drain reason=\"This would worK\"\n",
    "sinfo --N --long --cluster lyoko\n",
    "\n",
    "# undrain the node\n",
    "scontrol --cluster lyoko update nodename=ALL state=resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0432e5e-5661-4d08-ba40-3cbc8992780d",
   "metadata": {},
   "source": [
    "## Federation\n",
    "Federation is another level of cluster integration on top of multi-cluster. When a federation of clusters is created, sibling jobs is created on all or some federation members whenever a new job is submitted, so the job could start earlier if resource is available on another cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d96e2-de51-4a45-8bac-11a300c013cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Federation\n",
    "sacctmgr show clusters\n",
    "sacctmgr -i add federation world_without_danger\n",
    "# two ways of adding cluster to federation\n",
    "sacctmgr -i modify federation world_without_danger set clusters+=slurm-lab\n",
    "sacctmgr -i modify cluster lyoko set federation=world_without_danger\n",
    "\n",
    "# show federation\n",
    "sacctmgr show federation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3e4f2-3aa1-4505-bb32-930746230a92",
   "metadata": {},
   "source": [
    "### Job Submission and Slurm commands\n",
    "Job submitted to the federation has a globally unique jobid. Job's original cluster is distinguishable from the prefix of the jobid. By default slurm command shows information of local cluster only, use flag `--federation` to show federation related information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57788e-06d3-4d5f-b745-29ac13b4ee17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submittion 3 jobs local, expecting the 3rd job will be executed on the remote cluster\n",
    "jobid_0=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "jobid_1=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "jobid_2=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "\n",
    "# submit a job from cluster lyoko, expecting different prefix\n",
    "jobid_3=$(ssh slurm-lab-master-lyoko sbatch --ntasks=2 --parsable --time 00:05:00 tutorials/endless-checksum-mpi.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d6bc7-fb15-476e-90d8-2b872ae1ca09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set -x\n",
    "# show global job queue\n",
    "squeue -la --federation\n",
    "squeue -la --federation -M slurm-lab,lyoko\n",
    "\n",
    "# all available queue and nodes\n",
    "sinfo --long --federation\n",
    "sinfo --N --long --federation\n",
    "\n",
    "set +x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4e2da-d2e3-43d7-ac20-0106058166db",
   "metadata": {},
   "source": [
    "Note that for the third job submitted locally (on slurm-lab), its sibling job is started on remote cluster lyoko. The original job is in \"REVOKED\" state, and will not start on the original cluster. \n",
    "\n",
    "The job submitted via ssh on cluster lyoko's master has a differet jobid prefix, so job ids are globally unique, and still you can tell which cluster a job is originated from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e743f6-6803-4868-95a7-a08947b79d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scancel ${jobid_0} ${jobid_1} ${jobid_2} ${jobid_3}\n",
    "squeue -la --federation -M slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d576d-cd6c-4d0d-9d66-e9823c261ed1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Default to federation view\n",
    "By setting [FederationParameters](/doc/slurm.conf.html#OPT_FederationParameters) in slurm.conf, slurm commands will now default to show federated view. You don't have to set this variable on all the host, but it is always recommanded to keep slurm.conf consistent across all the nodes in a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3ff6a-469b-4b49-abcc-47e597530858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add parameter to local slurm.conf only\n",
    "ansible -m lineinfile -a 'path=/etc/slurm/slurm.conf regexp=\"^.*FederationParameters=.*$\" line=\"FederationParameters=fed_display\"' localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2593eab-62c1-426d-9ea6-82b6541aa97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submittion 3 jobs local, expecting the 3rd job will be executed on the remote cluster\n",
    "jobid_0=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "jobid_1=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "jobid_2=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "\n",
    "# submit a job from cluster lyoko, expecting different prefix\n",
    "jobid_3=$(ssh slurm-lab-master-lyoko sbatch --ntasks=2 --parsable --time 00:05:00 tutorials/endless-checksum-mpi.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eda26a-348c-4520-b0cd-3992539eeaed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set -x\n",
    "# show global job queue\n",
    "squeue -la\n",
    "squeue -la -M slurm-lab,lyoko\n",
    "\n",
    "# all available queue and nodes\n",
    "sinfo --long \n",
    "sinfo --N --long\n",
    "\n",
    "set +x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85156a-d3fb-4287-832e-3902490797a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scancel ${jobid_0} ${jobid_1} ${jobid_2} ${jobid_3}\n",
    "squeue -la -M slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317eff97-a9ef-4203-ae3f-98d5cc735dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# revert the change\n",
    "ansible -m lineinfile -a 'path=/etc/slurm/slurm.conf regexp=\"^.*FederationParameters=.*$\" state=\"absent\"' localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
