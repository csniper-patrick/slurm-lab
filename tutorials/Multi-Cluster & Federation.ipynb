{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1beffcb-8a59-4f27-84e3-d4a3908a9511",
   "metadata": {},
   "source": [
    "# Multi-Cluster & Federation\n",
    "Multi-cluster operation and federation are advanced features of Slurm. When multiple clusters are connected to the same slurmdb account services, multi-cluster operation allows job submission, monitoring, and configuration changes across clusters from the clients of any one of the clusters. On top of that, when a federation is configured, jobs submitted to a cluster member can be replicated to all or a subset of other clusters in the federation as sibling jobs, and a unified job ID space is used to uniquely identify a job globally. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60116f27-791b-42c2-bc32-70ea5f329874",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "The architecture of this container lab is expanded for trying out the multi-cluster and federation features.  \n",
    "![multi-cluster-and-federation](./multi-cluster-and-federation.drawio.svg)  \n",
    "\n",
    "A new cluster, \"lyoko\", is added to the architecture and will use the existing slurmdbd services hosted on the containers slurm-lab-master-[1-2]. You can do this by uncommenting the corresponding objects in  `./compose.dev.yml` and updating the cluster.\n",
    "```\n",
    "podman compose -f compose.dev.yml up -d \n",
    "```\n",
    "At this point, multi-cluster operation is enabled automatically, as they both use the same slurmdbd service. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57b51ff-3ab6-4db6-bc52-a77088d2263d",
   "metadata": {},
   "source": [
    "## Multi-Cluster Operation\n",
    "Although this client container is only a part of the slurm-lab cluster, the slurm command can be used to query, control, and interact with the other cluster (lyoko) that uses the same slurmdbd service with the `-M` or `--clusters` option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8ba63-a464-490a-88a0-f9f9363a4df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sinfo --long --clusters slurm-lab,lyoko\n",
    "sinfo --N --long --clusters slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdae471-2b9b-4265-bd0f-c20f2a0d01f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "squeue -la --clusters slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c54f20-3a3b-4579-8331-aa7496b5f707",
   "metadata": {},
   "source": [
    "### Job submission\n",
    "For job submission, you can list one or more clusters. One job will be submitted to the cluster that is estimated to start the job the earliest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab630c-71ed-42c5-b538-457d6481a4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit one job to slurm-lab, the local cluster\n",
    "jobid_0=$( sbatch --ntasks=2 --parsable --time 00:10:00 endless-checksum-mpi.sh )\n",
    "\n",
    "# submit one job to lyoko, the remote cluster\n",
    "# parsable output format: <jobid>;<cluster>\n",
    "IFS=';' read -r jobid_1 cluster_1 < <( sbatch --cluster lyoko --ntasks=2 --parsable --time 00:10:00 endless-checksum-mpi.sh )\n",
    "\n",
    "# submit one job to multi cluster, see where it lands\n",
    "# parsable output format: <jobid>;<cluster>\n",
    "IFS=';' read -r jobid_2 cluster_2 < <( sbatch --cluster slurm-lab,lyoko --ntasks=2 --parsable --time 00:10:00 endless-checksum-mpi.sh )\n",
    "\n",
    "cat <<EOF\n",
    "Job ${jobid_0} submitted to local cluster (slurm-lab)\n",
    "Job ${jobid_1} submitted to remote cluster (${cluster_1})\n",
    "Job ${jobid_2} submitted to earliest available cluster ${cluster_2}\n",
    "EOF\n",
    "\n",
    "squeue -la --clusters slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f9719-de8c-4a74-8da9-cdaf18e84389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cancel local job\n",
    "scancel ${jobid_0}\n",
    "\n",
    "# cancel job on specific cluster\n",
    "scancel --clusters ${cluster_1} ${jobid_1}\n",
    "scancel --clusters ${cluster_2} ${jobid_2}\n",
    "\n",
    "# check queue\n",
    "squeue -la --clusters slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9a4bc-6fb5-4f54-aef1-c90a5e4f6b9f",
   "metadata": {},
   "source": [
    "IMPORTANT: If multiple clusters are given, `scancel` will cancel jobs with the specified job ID on ALL of them. Job IDs are NOT unique!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9acfda0-2c1e-46de-9c88-29d087de7c33",
   "metadata": {},
   "source": [
    "### scontrol\n",
    "For `scontrol`, it is a little bit different. ONLY ONE cluster can be specified in the option; otherwise, a fatal error will occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcca80f-f27f-47b3-9bbd-d2e3a6b738b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example of fatal error\n",
    "scontrol --clusters slurm-lab,lyoko update nodename=ALL state=drain reason=\"this cmd is going to fail\"\n",
    "\n",
    "# draing nodes in the remote cluster \n",
    "scontrol --cluster lyoko update nodename=ALL state=drain reason=\"This would work\"\n",
    "sinfo --N --long --cluster lyoko\n",
    "\n",
    "# undrain the node\n",
    "scontrol --cluster lyoko update nodename=ALL state=resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0432e5e-5661-4d08-ba40-3cbc8992780d",
   "metadata": {},
   "source": [
    "## Federation\n",
    "Federation is another level of cluster integration on top of multi-cluster. When a federation of clusters is created, sibling jobs are created on all or some federation members whenever a new job is submitted, so the job can start earlier if resources are available on another cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d96e2-de51-4a45-8bac-11a300c013cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Federation\n",
    "sacctmgr show clusters\n",
    "sacctmgr -i add federation world_without_danger\n",
    "# two ways of adding cluster to federation\n",
    "sacctmgr -i modify federation world_without_danger set clusters+=slurm-lab\n",
    "sacctmgr -i modify cluster lyoko set federation=world_without_danger\n",
    "\n",
    "# show federation\n",
    "sacctmgr show federation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3e4f2-3aa1-4505-bb32-930746230a92",
   "metadata": {},
   "source": [
    "### Job Submission and Slurm commands\n",
    "A job submitted to the federation has a globally unique job ID. The job's original cluster is distinguishable by the prefix of the job ID. By default, the slurm command shows information for the local cluster only. Use the `--federation` flag to show federation-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57788e-06d3-4d5f-b745-29ac13b4ee17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submittion 3 jobs local, expecting the 3rd job will be executed on the remote cluster\n",
    "jobid_0=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "jobid_1=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "jobid_2=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "\n",
    "# submit a job from cluster lyoko, expecting different prefix\n",
    "jobid_3=$(ssh slurm-lab-master-lyoko sbatch --ntasks=2 --parsable --time 00:05:00 tutorials/endless-checksum-mpi.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d6bc7-fb15-476e-90d8-2b872ae1ca09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set -x\n",
    "# show global job queue\n",
    "squeue -la --federation\n",
    "squeue -la --federation -M slurm-lab,lyoko\n",
    "\n",
    "# all available queue and nodes\n",
    "sinfo --long --federation\n",
    "sinfo --N --long --federation\n",
    "\n",
    "set +x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4e2da-d2e3-43d7-ac20-0106058166db",
   "metadata": {},
   "source": [
    "Note that for the third job submitted locally (on slurm-lab), its sibling job is started on the remote cluster lyoko. The original job is in the \"REVOKED\" state and will not start on the original cluster. \n",
    "\n",
    "The job submitted via SSH on the cluster lyoko's master has a different job ID prefix, so job IDs are globally unique, and you can still tell which cluster a job is originated from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e743f6-6803-4868-95a7-a08947b79d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scancel ${jobid_0} ${jobid_1} ${jobid_2} ${jobid_3}\n",
    "squeue -la --federation -M slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d576d-cd6c-4d0d-9d66-e9823c261ed1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Default to federation view\n",
    "By setting [FederationParameters](/doc/slurm.conf.html#OPT_FederationParameters) in `slurm.conf`, the slurm commands will now default to showing the federated view. You don't have to set this variable on all the hosts, but it is always recommended to keep `slurm.conf` consistent across all the nodes in a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3ff6a-469b-4b49-abcc-47e597530858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add parameter to local slurm.conf only\n",
    "ansible -m lineinfile -a 'path=/etc/slurm/slurm.conf regexp=\"^.*FederationParameters=.*$\" line=\"FederationParameters=fed_display\"' localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2593eab-62c1-426d-9ea6-82b6541aa97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submittion 3 jobs local, expecting the 3rd job will be executed on the remote cluster\n",
    "jobid_0=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "jobid_1=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "jobid_2=$(sbatch --ntasks=2 --parsable --time 00:05:00 endless-checksum-mpi.sh)\n",
    "\n",
    "# submit a job from cluster lyoko, expecting different prefix\n",
    "jobid_3=$(ssh slurm-lab-master-lyoko sbatch --ntasks=2 --parsable --time 00:05:00 tutorials/endless-checksum-mpi.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eda26a-348c-4520-b0cd-3992539eeaed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set -x\n",
    "# show global job queue\n",
    "squeue -la\n",
    "squeue -la -M slurm-lab,lyoko\n",
    "\n",
    "# all available queue and nodes\n",
    "sinfo --long \n",
    "sinfo --N --long\n",
    "\n",
    "set +x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85156a-d3fb-4287-832e-3902490797a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scancel ${jobid_0} ${jobid_1} ${jobid_2} ${jobid_3}\n",
    "squeue -la -M slurm-lab,lyoko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317eff97-a9ef-4203-ae3f-98d5cc735dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# revert the change\n",
    "ansible -m lineinfile -a 'path=/etc/slurm/slurm.conf regexp=\"^.*FederationParameters=.*$\" state=\"absent\"' localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d72ed-74b1-4cdd-9adf-0a949efd2361",
   "metadata": {},
   "source": [
    "### Cluster state in federation\n",
    "Federation member can have 4 different states\n",
    "  - **ACTIVE**: Accepting job \n",
    "  - **INACTIVE**: Not running or accepting jobs\n",
    "  - **DRAIN**: Not accepting new job. but job still running.\n",
    "  - **DRAIN+REMOVE**: Set cluster to drain, and remove from federation once it is drained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f1ca5-e57d-4b04-8717-dc66bac66f39",
   "metadata": {},
   "source": [
    "### Removing cluster from federation\n",
    "Removing cluster lyoko and slurm-lab from federation world_without_danager by setting fedstate to \"DRAIN+REMOVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630dea4-8fd7-4720-b172-5d4a53ff7bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sacctmgr -i modify cluster lyoko set fedstate=DRAIN+REMOVE\n",
    "# sacctmgr -i modify cluster slurm-lab set fedstate=DRAIN+REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd4129-1062-4799-a4cd-07891565c74d",
   "metadata": {},
   "source": [
    "#### Deleting federation\n",
    "Deleting federation world_without_danager. It is better to drain all cluster before delete the federation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6677bcc0-3ace-43d1-b719-4a3df20dc057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sacctmgr -i delete federation world_without_danger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
