{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c70ca3-7af7-436f-a6cd-bfbc48a109d5",
   "metadata": {},
   "source": [
    "# Administrator Guide\n",
    "In this administrator guide, some essential admin command and config files will be introduced. Hopefully I can demostrate some of the frequently use operation with some example which you can run right on this notebook. Examples in this notebook requires more compute node, you need to scale up the compute containers to 4.\n",
    "```\n",
    "podman compose -f compose.dev.yml up -d --scale compute=4 --no-recreate\n",
    "```\n",
    "If not all 4 compute show up in `sinfo`, try restarting individual compute containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f4cdd-b639-4e89-939a-8dbb8eaef608",
   "metadata": {},
   "source": [
    "### Useful admin commands\n",
    "| Command | Usage |\n",
    "|---------|-------|\n",
    "| scontrol | Make configuration changes and query status/property of Slurm in runtime. |\n",
    "| sacctmgr | Managing interface of the slurm accounting service, Cluster, account, user, federation, Quality of Services .etc can be managed from here |\n",
    "| sacct | Query job accounting record, not really making any configuration changes, but a good tool for observing the usage |\n",
    "\n",
    "### Config Files\n",
    "The general rule is that if a certain configuration change is made to persist across boot/service restart/slurm reconfigure, you need to put them in these files. You can sync this file across the cluster, put them in a location that is shared across the cluster, or use the configless option.  \n",
    "| Conf file | Usage |\n",
    "|-----------|-------|\n",
    "| slurm.conf | Initial slurm configuration at start up. |\n",
    "| slurmdbd.conf | Initial slurmdb configuration at start up. |\n",
    "| cgroup.conf | cgroup configuration |\n",
    "| gres.conf | \"General resource\" configuration. GPU configuration is defined here |\n",
    "| topology.conf | Describe network topology, helping slurm to select better combination of nodes when allocating resource |\n",
    "\n",
    "After updating these files across the cluster you can either run reconfigure sub-command or restart the daemons to make the changes effective. Note that some changes requires daemon restart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d665a5a-1afa-46e5-84bf-261b4e5b7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo sacctmgr reconfigure\n",
    "sudo scontrol reconfigure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe738b5-517d-4d14-9637-78b13f918770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set -x\n",
    "# restart services on different nodes using ansible\n",
    "ansible-inventory --graph \n",
    "\n",
    "# restart slurmdbd-host\n",
    "ansible -m systemd -a \"name=slurmdbd  state=restarted enabled=yes\" slurmdbd_host\n",
    "\n",
    "# restart slurmctld-host\n",
    "ansible -m systemd -a \"name=slurmctld state=restarted enabled=yes\" slurmctld_host\n",
    "\n",
    "# restart slurmd-host\n",
    "ansible -m systemd -a \"name=slurmd    state=restarted enabled=yes\" slurmd_host\n",
    "\n",
    "# restart slurmrestd-host\n",
    "ansible -m systemd -a \"name=slurmdbd  state=restarted enabled=yes\" slurmrestd_host\n",
    "\n",
    "set +x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ca0de-2ae3-4b03-9259-a08fab8683c9",
   "metadata": {},
   "source": [
    "## Create Slurm Account and User automatically at first login\n",
    "It is a good practice to set `AccountingStorageEnforce=associations,limits,qos`, there are many limits, restriction you can configure via slurm accounting DB. And having these flags being set, any user will have to have an account and user created in the slurm accounting database before they can submit jobs.  \n",
    "It is just an extra step when creating the linux account if your cluster is using local account or a Directory Services that is dedicated to the cluster. But if your cluster is connedted to the organization AD, LDAP, FreeIPA, etc. , where there are lots of account and changes from time to time, and the Slurm account and user creation could become troublesome.  \n",
    "One way of automating this process is to utilize the pam_exec.so module, it can be configured to execute a command or script everytime a user login. The difference to `/etc/profile.d/` scripts is that scripts under profile.d is sourced by the user's shell session, it is bounded to what the user himself can do, and a normal user certainly cannot create them self a slurm account and Slurm user. pam_exec.so on the other hand execute the script with system/root before user's session starts, and hence it can create slurm account and user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fb04f-95b5-4d0e-a9c4-75ddcae53b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pam config\n",
    "grep pam_exec.so /etc/pam.d/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c81f76-2127-4c06-9fff-6284d5a03fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# account and user creation script\n",
    "ls -l /etc/slurm/create-account-user.sh\n",
    "cat /etc/slurm/create-account-user.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a79b4e-aebb-4f76-8e19-1bb345e7321e",
   "metadata": {},
   "source": [
    "## Admin level\n",
    "If you want to admin action via scontrol and sacctmgr without escalating privileges, or you trust someone with Slurm administration but not the rest of your system. you can promote a slurm user to one of the following administrator level.  \n",
    "| Admin level | Description |\n",
    "|-------------|-------------|\n",
    "| [Admin](/doc/user_permissions.html#admin) | You can run scontrol, sacctmgr command as if you are root. |\n",
    "| [Operator](/doc/user_permissions.html#operator) | Modify any slurm database object |\n",
    "| [Coordinator](/doc/user_permissions.html#coord) | Special user role to a slurm account, able to manage data and object for all user of that account. |\n",
    "\n",
    "`PrivateData` parameter in slurm.conf can restrict data readable by user. However, users with different admin level have atleast the permission to read/write the following objects, regardless of PrivateData restriction.  \n",
    "\n",
    "|   | Admin | Operator | Coordinator |\n",
    "|---|-------|----------|-------------|\n",
    "| Jobs | Read-Write | Read-Write | Read-Write (specific account) |\n",
    "| Reservation | Read-Write | Read-Write |  |\n",
    "| Partition | Read-Write | Read-Only |  |\n",
    "| Node | Read-Write | Read-Only |  |\n",
    "\n",
    "Reference:\n",
    "* [Scontrol - Authorization](/doc/scontrol.html#SECTION_AUTHORIZATION)\n",
    "* [User Permission](/doc/user_permissions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5eb952-849a-4239-b6c7-1353f456b83f",
   "metadata": {},
   "source": [
    "### Promoting user to be coordinator of an account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ea563-835c-4d49-a4d2-cc38263f5cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Promote current user to coordinator\n",
    "sudo sacctmgr -i create coordinator account=$(id -gn) name=$(whoami)\n",
    "sacctmgr show account WithCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1131649-e104-47a0-8d0a-e68c1b5d3327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete coordinator\n",
    "sacctmgr -i delete coordinator account=$(id -gn) name=$(whoami)\n",
    "sudo sacctmgr show account WithCoord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c077ac50-1179-4995-a355-ac67156d3642",
   "metadata": {},
   "source": [
    "### Promoting user to be Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59353b8c-55e1-4e18-aaff-a09efc571588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sudo sacctmgr -i modify user $(whoami) set AdminLevel=operator\n",
    "sacctmgr show user user=$(whoami)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7a97d-526b-47d7-8065-17ad320773b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove admin level, become normal slurm user\n",
    "sudo sacctmgr -i modify user $(whoami) set AdminLevel=None\n",
    "sacctmgr show user user=$(whoami)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c80a0e-5584-4690-bcc0-29c58e84d59c",
   "metadata": {},
   "source": [
    "### Promoting user to be Administrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b15bdd-29c6-44fa-bf59-21c7a7137d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sudo sacctmgr -i modify user $(whoami) set AdminLevel=admin\n",
    "sacctmgr show user user=$(whoami)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520e94c-3227-4096-9a82-d2fcf9ee4c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eg. draining nodes\n",
    "scontrol update nodename=ALL state=drain reason=\"just practice\"\n",
    "sinfo --N --long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859467fa-ed3e-416c-825c-599e68c645b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eg. putting them back\n",
    "scontrol update nodename=ALL state=resume\n",
    "sinfo --N --long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb28e3-00c8-46cd-ab86-d5627e074d94",
   "metadata": {},
   "source": [
    "## Modify/extend time limit of a running job\n",
    "If a time limited job is running slower than expect and it is approaching time limit, you as admin do have the power to extend it.  \n",
    "(If you are a user, now you know this, but please don't bother your admin with these unless the job is absolutely critical.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257fa14-222f-4c1d-8300-612e0b96a0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit a 10 min job\n",
    "jobid=$(sbatch --ntasks=1 --parsable --time 00:10:00 endless-checksum-mpi.sh)\n",
    "scontrol show job ${jobid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a38b7-84e5-4f74-87a6-42d3accc47c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extend the job to 20 min\n",
    "scontrol update JobId=${jobid} TimeLimit=00:20:00\n",
    "scontrol show job ${jobid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d8c06-5d2a-4ec5-9a8b-77931b7a7d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extend the job for 10 more min\n",
    "scontrol update JobId=${jobid} TimeLimit+=00:10:00\n",
    "scontrol show job ${jobid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd36b1-233e-4cf1-9533-c222ee34422e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# removing time limit\n",
    "scontrol update JobId=${jobid} TimeLimit=INFINITE\n",
    "scontrol show job ${jobid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258245f3-8334-4f25-9016-cf237af17ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove the job\n",
    "scancel ${jobid}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df1eb2-b147-4cb1-ba49-1a7cd33dce32",
   "metadata": {},
   "source": [
    "## Drain, Resume nodes\n",
    "Changing node state is a very common operation, eg. if you have identify a faulty node, or you want to perform maintenance task on the node.  \n",
    "If you don't want the node to accept new jobs, set it to drain and you must provide a reason. The node will stop accepting new jobs. State is \"draining\" when there is running, \"drained\" when no job. If you want to put the node back, set state to \"resume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43427c42-b5af-42bd-9591-03a704947c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit a dummy job\n",
    "jobid=$( sbatch --ntasks=1 --parsable endless-checksum-mpi.sh )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b157773-c633-4ac2-ac55-b316b237f642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait for the job to start\n",
    "squeue -l -j ${jobid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8fe32-2cbf-4af6-b291-2aaf9c85964d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drain all node and see the different in states, ALL for draining all node\n",
    "scontrol update NodeName=ALL State=drain Reason=\"drain demo\"\n",
    "sinfo --N --long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ceed0-ae57-42e4-b104-98d52074587d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cancel the job\n",
    "scancel ${jobid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068ac3f-419b-45eb-a70f-4c2c1a4a5296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Observe that the draining node become drained\n",
    "sinfo --N --long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fefbaf-4a27-42bb-a381-b4ec3f0f0047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put first drained node, if any, on the list back to production\n",
    "node=$(sinfo --noheader --N --long --state=drain | awk '{print $1}' | head -1)\n",
    "[[ -n ${node} ]] && scontrol update nodename=${node} state=resume\n",
    "sinfo --N --long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144970c5-b979-4da5-8f12-ae5205ae08e4",
   "metadata": {},
   "source": [
    "## Create, Remove, Change state of Partition \n",
    "Using the scontrol command, you can create, remove, and change state of partitions in runtime. However these operations are ephemeral, you need to put the equivalence into slurm.conf to make it persistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f65503-07ef-4895-895a-a02fd2bee568",
   "metadata": {},
   "source": [
    "### Create Partition\n",
    "To create a partition, we need to specify at least the name. You can also include some partition properties, but you can always modify them afterward.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa1fec-5d75-448f-b20f-462689b43e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eg. creating a new partition name DEV, setting maximum time limit at 1 hr\n",
    "# just pick a node\n",
    "node=$(sinfo --noheader --N --long | awk '{print $1}' | head -1)\n",
    "scontrol create PartitionName=DEV Nodes=${node} MaxTime=01:00:00\n",
    "scontrol show partition DEV\n",
    "sinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882f858-bd72-453f-b1a0-1843546d2e64",
   "metadata": {},
   "source": [
    "### Change state and modify property of partition\n",
    "For partition there are 4 possible states:  \n",
    "\n",
    "|   | Accepting job | Rejecting job |\n",
    "|---|---------------|---------------|\n",
    "| Dispatching job | UP   | DRAIN    |\n",
    "| Holding job     | DOWN | INACTIVE |\n",
    "\n",
    "Changing state is just like changing other properties, using \"scontrol update\" subcommand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45368e0-b219-4f40-8ae4-2388f5379473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting DEV state to DOWN and submit a jobs \n",
    "scontrol update PartitionName=DEV state=DOWN\n",
    "sbatch --ntasks=1 --parsable --partition=DEV endless-checksum-mpi.sh\n",
    "# Observe that the jod has been submitted, but won't execute\n",
    "squeue --partition DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bd00b-dced-4cce-97c1-6082c6a1dc25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting DEV state to INACTIVE/DRAIN and submit a job, but fail\n",
    "# INACTIVE\n",
    "scontrol update PartitionName=DEV state=INACTIVE\n",
    "sbatch --ntasks=1 --parsable --partition=DEV endless-checksum-mpi.sh\n",
    "\n",
    "# DRAIN\n",
    "scontrol update PartitionName=DEV state=DRAIN\n",
    "sbatch --ntasks=1 --parsable --partition=DEV endless-checksum-mpi.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf8562-1808-46b2-89ca-c44b49f27721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# job submission failed, but the job submitted before is now running\n",
    "squeue --partition DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f971527-4f9c-4722-b829-5db3da46037b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Allowing the overscribe in the partition, and try overwhelm the partition\n",
    "scontrol update PartitionName=DEV OverSubscribe=FORCE State=UP DefMemPerNode=1024\n",
    "# clear all job first\n",
    "squeue --partition DEV --noheader | awk '{print $1}' | xargs scancel\n",
    "for i in $(seq 5) ; do\n",
    "sbatch --ntasks=1 --parsable --partition=DEV endless-checksum-mpi.sh\n",
    "sleep 1\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4de02-452d-440a-bd07-e4382e4ce1df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "squeue --partition DEV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c716b2c-25b6-4ec0-9a22-88294f870b59",
   "metadata": {},
   "source": [
    "### Delete partition\n",
    "Before deleting a partition, you must clear the partition (no running or pending jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ddeff-92db-468f-a617-928e6455068e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set partition to Drain\n",
    "scontrol update PartitionName=DEV State=DRAIN\n",
    "# cancel all jobs in the partition\n",
    "squeue --partition DEV --noheader | awk '{print $1}' | xargs scancel\n",
    "scontrol delete PartitionName=DEV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640255ba-8759-49c4-a1f7-e67dda6d038b",
   "metadata": {},
   "source": [
    "## Move node around Partitions\n",
    "In a production cluster, you may have a partition serving critical jobs, and some other partition serving less important jobs. You may need to move nodes around when a node is down to make sure the mission critical partition has enough resource.  \n",
    "Unfortunately \"scontrol\" doesn't support \"+=\" and \"-=\" operations, so we will need to deal with the whole list everytime. We could define some shell function to help though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d0987-0727-4a4f-95ca-8966d6ad5758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just one way of doing it\n",
    "\n",
    "# bash func to add a node to partition\n",
    "# Usage: partAddNode <partition name> <list of nodes>\n",
    "partAddNodes () {\n",
    "    # Check if partition exist\n",
    "    [[ -n $(scontrol show partition ${1} --oneliner | grep \"PartitionName=${1}\" ) ]] || return 1\n",
    "    # Get current node list\n",
    "    current_nodelist=$(sinfo --N --long --partition ${1} --noheader | awk '{print $1}')\n",
    "    new_nodelist=$( echo ${current_nodelist} $(scontrol show hostname ${2}) | tr ' ' '\\n' | sort | uniq | paste -s -d\",\")\n",
    "    # Update partition node list\n",
    "    scontrol update PartitionName=${1} Nodes=${new_nodelist}\n",
    "}\n",
    "\n",
    "# bash func to remove a node to partition\n",
    "# Usage: partDelNode <partition name> <list of nodes>\n",
    "partDelNodes () {\n",
    "    # Check if partition exist\n",
    "    [[ -n $(scontrol show partition ${1} --oneliner | grep \"PartitionName=${1}\" ) ]] || return 1\n",
    "    # Get current node list\n",
    "    current_nodelist=$(sinfo --N --long --partition ${1} --noheader | awk '{print $1}')\n",
    "    remove_nodes=$(scontrol show hostname ${2})\n",
    "    new_nodelist=$( echo ${current_nodelist} ${remove_nodes} ${remove_nodes} | tr ' ' '\\n' | sort | uniq -u | paste -s -d\",\")\n",
    "    # Update partition node list\n",
    "    scontrol update PartitionName=${1} Nodes=${new_nodelist}\n",
    "}\n",
    "\n",
    "# bash function to move nodes from A to B\n",
    "# Usage: moveNodesToPart <src partition> <dest partition> <list of nodes>\n",
    "moveNodesToPart (){\n",
    "    partAddNodes ${2} ${3} && partDelNodes ${1} ${3}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200efc1-1109-4eb9-afc7-f05aee95b100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup Part_A and Part_B for example\n",
    "scontrol create PartitionName=Part_A Nodes=All\n",
    "scontrol create PartitionName=Part_B\n",
    "sinfo --long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1afc83-89c0-4e31-bfc0-2682594f6717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Move first node in Part_A to Part_B\n",
    "moveNodesToPart Part_A Part_B $(sinfo --noheader --N --long --partition Part_A | awk '{print $1}' | head -1)\n",
    "sinfo --long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93686539-ed87-4ce0-ae8e-06b36cb796cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete demo partitions\n",
    "scontrol delete PartitionName=Part_A\n",
    "scontrol delete PartitionName=Part_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94384150-b4c4-4438-876b-7db68b77e374",
   "metadata": {},
   "source": [
    "## Floating Partition\n",
    "Instead of having your operator actively swapping nodes, there is another way to maintain certain no. of healthy node in the mission critical partition, [floating partition](/doc/qos.html#partition). You can assign ALL suitable nodes to the mission critical, possibly sharing some nodes with other partitions, and then define a Quality-of-Services(QOS) to limit the ammount of nodes it can use.  \n",
    "For example, if we include all node in the mission critical partition PROD, sharing 3 node with development partition DEV, but only allowing partition PROD to use at most 5 nodes at a time. When All nodes are normal, the PROD will not use more than the first 5 nodes, but when some node failed in the first 5 nodes, the PROD partition can automatically \"steal\" some nodes from the DEV partition and keep run at 5 node capacity. If priority factor is setup properly, the DEV partition will suffer from this node failure incident instead of the PROD partition.  \n",
    "![floating-partition](floating-partition.drawio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535458e-d5a2-4d60-bdd7-5e95c8c3e7ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure \"AccountingStorageEnforce\" includes qos\n",
    "scontrol show config | grep -iE ^AccountingStorageEnforce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e589a0-fa0f-4d79-9f26-d960766afa31",
   "metadata": {},
   "source": [
    "### Create Floating Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903418c9-31d4-4fc9-b915-099030153a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create partition QoS\n",
    "sacctmgr -i add qos qos_prod set GrpTres=node=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abadc65c-9b69-426d-8866-424712a95cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create partition PROD and DEV.\n",
    "# PROD get all nodes\n",
    "scontrol create PartitionName=PROD MaxTime=00:05:00 QOS=qos_prod Nodes=ALL\n",
    "# DEV share the last 2 node with PROD\n",
    "scontrol create PartitionName=DEV  MaxTime=00:05:00 Nodes=$(sinfo --noheader --N --long --partition PROD | awk '{print $1}' | tail -2 | paste -s -d\",\")\n",
    "# show partitions\n",
    "sinfo --long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe95cc7-2638-4a37-9f79-f6a6f6e71b6e",
   "metadata": {},
   "source": [
    "### Simulate Normal Case\n",
    "Lets submit 3 1-node jobs to PROD, and 1 1-node job to DEV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f0a80e-47bc-405c-9d9e-0a5f0e964873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3 jobs to PROD\n",
    "seq 3 | xargs -i sbatch --nodes=1 --ntasks-per-node=2 --parsable --partition=PROD endless-checksum-mpi.sh\n",
    "# 1 jobs to DEV\n",
    "seq 1 | xargs -i sbatch --nodes=1 --ntasks-per-node=2 --parsable --partition=DEV endless-checksum-mpi.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed48f03-1945-4645-b7e5-bd9f317b9b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check job queue\n",
    "squeue -la --sort=i\n",
    "sinfo --long --partition=PROD,DEV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb99466-c843-4ba1-99ff-7d3c3b3e39b2",
   "metadata": {},
   "source": [
    "Note that the 3rd job in PROD is \"pending\" with reason `QOSGrpNodeLimit` despite having 1 idle node in the partition.  \n",
    "(BTW this is one downside of having a floating partition if normal user are able to see partition node states. They might wonder why their job is not starting despite having idle nodes. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322cb553-989a-4150-8a5f-84ffd2271811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clear all jobs\n",
    "squeue --noheader --partition PROD,DEV | awk '{print $1}' | xargs scancel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d920f0b2-15e0-45a8-b176-22a179b52bcf",
   "metadata": {},
   "source": [
    "### Simulate Node Failure\n",
    "Now we simulate node failure by draining a node in PROD, and then submit some jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c24ac-8b9f-4e4e-bb56-bdf39e8f2075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draining first node in PROD\n",
    "scontrol update NodeName=$(sinfo --N --long --noheader --partition PROD | awk '{print $1}' | head -1) State=Drain Reason=\"Node Failure\"\n",
    "sinfo --long --partition=PROD,DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e680907-c161-465d-9d06-e418039ef906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3 jobs to PROD\n",
    "seq 3 | xargs -i sbatch --nodes=1 --ntasks-per-node=2 --parsable --partition=PROD endless-checksum-mpi.sh\n",
    "# 1 jobs to DEV\n",
    "seq 2 | xargs -i sbatch --nodes=1 --ntasks-per-node=2 --parsable --partition=DEV endless-checksum-mpi.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2f8c2-3ef5-4522-8d32-f36a5c296d03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check job queue\n",
    "squeue -la --sort=i\n",
    "sinfo --long --partition=PROD,DEV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d38acc-b29b-4f44-91bd-82f4c7998129",
   "metadata": {},
   "source": [
    "PROD is able to \"steal\" a node from DEV to maintain 2 node capacity. As a result, it is partition DEV suffer from the node failure, instead of the mission critical partition PROD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb638e83-c475-48b8-8256-63764879cf8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "squeue --noheader --partition PROD,DEV | awk '{print $1}' | xargs scancel\n",
    "# resume node\n",
    "scontrol update NodeName=$(sinfo --N --long --noheader --partition PROD --state drain | awk '{print $1}' | paste -s -d\",\") State=resume\n",
    "# delete partition \n",
    "scontrol delete PartitionName=PROD\n",
    "scontrol delete PartitionName=DEV\n",
    "# delete QoS\n",
    "sacctmgr -i delete qos name=qos_prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2fdb3-26a1-48b5-972f-438682bbd608",
   "metadata": {},
   "source": [
    "## Create and Manage Reservation\n",
    "There are many options for creating a reservations. For detail please refer to these 2 documents:\n",
    "1. [scontrol: reservation](/doc/scontrol.html#SECTION_RESERVATIONS---SPECIFICATIONS-FOR-CREATE,-UPDATE,-AND-DELETE-COMMANDS)\n",
    "2. [Advanced Resource Reservation Guide](/doc/reservations.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fad0b0-be44-45da-a31e-934ef07ccd6b",
   "metadata": {},
   "source": [
    "### Basic reservation for running job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4463a6-d777-4414-8830-3d04245ac309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reserve 1 node in debug for 1 hr, 5 min from now, for yourself\n",
    "resv_name=$(whoami)_resv_1\n",
    "scontrol create reservationname=${resv_name} user=$(whoami) partition=debug nodecnt=1 duration=60 starttime=$(date --date \"now + 5 min\" +\"%FT%T\" )\n",
    "scontrol show reservation ${resvname}\n",
    "sinfo --reservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6b328-9d4f-4b93-b328-8ac66c556bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit job using the reservation\n",
    "jobid=$(sbatch --nodes=1 --ntasks-per-node=2 --parsable --time 00:10:00 --partition debug --reservation ${resv_name} endless-checksum-mpi.sh)\n",
    "scontrol show job ${jobid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7adb5c-312f-4d7a-8319-e9488b807971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the job will start once the reservation become active\n",
    "sinfo --reservation\n",
    "squeue -la -j ${jobid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbb0bd-0165-4186-a45c-6a317b59bb31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# without special flags, reservation can only be made when resource is available, this should fail\n",
    "scontrol create ReservationName=fail_resv user=$(whoami) partition=debug \\\n",
    "    nodes=ALL duration=60 starttime=$(date --date \"now + 5 min\" +\"%FT%T\" )\n",
    "\n",
    "# remove the reservation just in case\n",
    "scontrol delete ReservationName=fail_resv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbab708-d7e3-4c74-ab40-77e298006a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scancel ${jobid}\n",
    "scontrol delete ReservationName=${resv_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4ec55-6559-4e32-b082-8c2e5a510990",
   "metadata": {},
   "source": [
    "#### Periodic Reservation\n",
    "If you need the reservation to repeat, you can use these flags:\n",
    "* [Daily](/doc/scontrol.html#OPT_DAILY)\n",
    "* [Hourly](/doc/scontrol.html#OPT_HOURLY)\n",
    "* [Weekday](/doc/scontrol.html#OPT_WEEKDAY)\n",
    "* [Weekend](/doc/scontrol.html#OPT_WEEKEND)\n",
    "* [weekly](/doc/scontrol.html#OPT_WEEKLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294efac-ecfc-4c06-9b9d-af372169d146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reserve 5 cores for 10 min repeat hourly, accessable by account lyoko\n",
    "resv_name=lyoko_hourly_5core\n",
    "scontrol create ReservationName=${resv_name} \\\n",
    "    flag=hourly account=lyoko partition=debug CoreCnt=5 \\\n",
    "    duration=5 starttime=$(date --date \"now + 1 min\" +\"%FT%T\") \n",
    "scontrol show reservation ${resv_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d92f3-1a86-4d6e-accf-aba9022986ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep watching, and observe that the reservation repeats after it ends\n",
    "sinfo --reservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a095e23-4af6-4dfd-a425-a5362f3e6138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete reservation\n",
    "scontrol delete ReservationName=${resv_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275110fe-ff8e-4974-ad89-f774e276437d",
   "metadata": {},
   "source": [
    "#### Magnetic and Flexible Reservation\n",
    "- A [magnetic](/doc/reservations.html#magnetic) reservation will be attached to a job when suitable, user don't need to specify the reservation id\n",
    "- A [flexible](/doc/reservations.html#flex) reservation allow job to use more resource than reserved when available. eg. use more core, or run beyond the reserved time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b6e75-1a5f-4162-9354-680ca34caf52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resv_name=jeremie_mag_flex\n",
    "start_time=$(date --date \"now + 1 min\" +\"%FT%T\")\n",
    "scontrol create ReservationName=${resv_name} \\\n",
    "    flag=magnetic,flex account=lyoko partition=debug Nodes=ALL \\\n",
    "    duration=10 starttime=${start_time}\n",
    "scontrol show reservation ${resv_name}\n",
    "\n",
    "# submit a job that should use the reservation automatically\n",
    "jobid=$(sbatch --nodes=1 --ntasks-per-node=2 --parsable --time 00:20:00 --partition debug --begin ${start_time} endless-checksum-mpi.sh)\n",
    "scontrol show job ${jobid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae245b0-a5e2-49d5-b331-75ae99a871e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# monitor until the reservation become active, the job should start. Then keep watching\n",
    "sinfo --reservation\n",
    "squeue -la -j ${jobid}\n",
    "scontrol show job ${jobid} | grep -i reservation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e20b7e-dec6-4dfd-ba4a-a4aaa42ff37b",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. When the reservation become active, the it is attached to the job automatically.\n",
    "2. Job time limit is longer than the reservation duration. The job is allowed to use the reservation because of the FLEX flag.\n",
    "3. Once the reservation end, it is detached from this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ed92f-3bdd-411f-938f-eb2dee66e88a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# once finish, clean up job and reservation\n",
    "scancel ${jobid}\n",
    "scontrol delete ReservationName=${resv_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98356f-6d57-469e-b9f6-5be87b736d30",
   "metadata": {},
   "source": [
    "#### Maintenance Reservation\n",
    "Instead of draining the nodes, setting up a maintenance Reservation is a more graceful way of draining the cluster for maintenance. Let's say you have scheduled a maintenance window, and you start draining the cluster at the start of the window, then you could be wasting large part of the window in waiting jobs to finish, or having to cancel the jobs. If you start draining it, then you wasted the computing power to run small jobs that can be completed before the window start.  \n",
    "If you create a Maintenance reservation instead, The reservation blocks any job that won't finish before the reservation start, and you can schedule it lond before the maintenance starts. One thing you need to be careful is that if your cluster runs lots of unlimited time jobs, those jobs will not be able to start once this reservation is placed (of course, cause they overlapped with the reservation), then you shoudl consider other method of clearing the cluster for maintenance.  \n",
    "Flags used for creating maintenance reservation are [MAINT](/doc/scontrol.html#OPT_MAINT) and [IGNORE_JOBS](/doc/scontrol.html#OPT_IGNORE_JOBS). MAINT allow the reservation to overlap with other reservation. IGNORE_JOBS allow the reservation to overlap with currently running jobs. Basically just allowing the reservation to be created anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335694b-618a-4141-8da6-258150f9f8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start an unlimited time job\n",
    "jobid=$(sbatch --nodes=1 --ntasks-per-node=2 --parsable endless-checksum-mpi.sh)\n",
    "echo wait for job ${jobid} to start\n",
    "while [[ -z $(squeue -j ${jobid} --noheader --state=running) ]] ; do \n",
    "    sleep 5 \n",
    "done\n",
    "squeue -j ${jobid} -la\n",
    "\n",
    "# create a dummy reservation\n",
    "scontrol create ReservationName=dummy_resv Account=lyoko NodeCnt=2 starttime=$(date --date \"now + 20 sec \" +\"%FT%T\") duration=60 \n",
    "\n",
    "# create a maintance reservation that overlap with both the job and dummy reservation\n",
    "scontrol create ReservationName=maint_resv flags=MAINT,IGNORE_JOBS User=root Nodes=All starttime=$(date --date \"now + 10 sec \" +\"%FT%T\") duration=60\n",
    "\n",
    "# create an unlimited job after the reservation has been created\n",
    "jobid2=$(sbatch --nodes=1 --ntasks-per-node=2 --parsable endless-checksum-mpi.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc09354-d6fc-40d5-9830-400596438678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "squeue -la\n",
    "sinfo -T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4ea4f-de7b-4f10-829b-8457d46e6295",
   "metadata": {},
   "source": [
    "The first unlimited time job will keep running until finish, but if there are not that many job like this just handle it case by case. Note that the second unlimited time job isn't able to start, because of the maintenance reservation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44526cb5-ce1c-4b65-a2ff-797bf619e767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up jobs and reservations\n",
    "scancel ${jobid} ${jobid2}\n",
    "scontrol delete ReservationName=dummy_resv\n",
    "scontrol delete ReservationName=maint_resv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9a2b4-6ba3-4423-b1ed-2e70bbd0b553",
   "metadata": {},
   "source": [
    "## Accounting\n",
    "From the `sacct` command, you can get many job metrics, useful for analysist. and report generation. \n",
    "ref: [sacct - manpage](/doc/sacct.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e8d07-ccf3-4862-945c-ea76ed83876d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set -x\n",
    "# job history since midnight (default)\n",
    "sacct\n",
    "\n",
    "# job history of a given range. eg. last since 3 hr ago to 1 hr ago\n",
    "sacct --starttime $(date --date \"now - 3 hour\" +\"%FT%T\") --endtime $(date --date \"now - 1 hour\" +\"%FT%T\")\n",
    "\n",
    "# don't show job steps\n",
    "sacct --allocation\n",
    "\n",
    "# show job step average resource usage\n",
    "sacct --format JobID,JobName,State,Partition,Account,AllocTRES,AveCPU,AveCPUFreq,AvePages,AveRSS\n",
    "\n",
    "# show job step peak resource usage\n",
    "sacct --format JobID,JobName,State,Partition,Account,AllocTRES,MaxPages,MaxPagesNode,MaxRSS,MaxRSSNode,MaxVMSize,MaxVMSizeNode\n",
    "\n",
    "sacct --format JobID,JobName,State,Partition,Account,AllocTRES,TRESUsageInAve%40,TRESUsageInMax%40\n",
    "set +x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1345c-29c3-4571-a67b-cf6f84856b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if certain job's runtime is beyond the specified range, you can use flag --truncate to align the data, and avoid double counting\n",
    "sacct --truncate --starttime $(date --date \"now - 10 min\" +\"%FT%T\") --format JobID,JobName,State,Partition,User,TRESUsageInMax%60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e4ed02-2697-4549-9ee8-a2b5156e05a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You could be seeing your own jobs by default, use --alluser to check history of more user. \n",
    "# What you can see is restricted by PrivateData attribute\n",
    "sacct --alluser --allocation --format JobID,JobName,State,Partition,User,TRESUsageInMax%60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd27126-9b3f-406b-b016-425fee70a00f",
   "metadata": {},
   "source": [
    "## Parsable Command Output\n",
    "squeue, sinfo, scontrol. sacct provide --parsable and --json flag for formatting output in more parsable format. This is useful in developping script around the slurm cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e06206-a0ba-47d5-9e55-2eac2b6a735d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sinfo --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc0d0d-ec37-4b36-a96f-49f46f7b8bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "squeue --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296f40f-4e1f-48e4-9ab2-378b976f1c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scontrol show partition --json\n",
    "scontrol show node --json\n",
    "scontrol show job --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f403fad1-4d52-49b0-aeeb-2a7136b0623c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sacct --json --starttime $(date --date \"now - 20 min\" +\"%FT%T\") --endtime $(date --date \"now - 10 min\" +\"%FT%T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943caa3-2e2f-4169-a80c-d14e4501f5aa",
   "metadata": {},
   "source": [
    "## User, Access Control, Authentication by Slurm\n",
    "In this section 3 useful tools provided by Slurm will be introduced: pam_slurm_adopt.so, nss_slurm, and auth/slurm a new AuthType. 3 new users are created under group `matrix` for this demo.\n",
    "  * `smith`: created everywhere\n",
    "  * `trinity`: created on this node (client) and 2 master nodes only\n",
    "  * `neo`: created on this node (client) only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2364a9-d762-4f96-a128-e7a53eeff980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create users\n",
    "cat playbooks/matrix-user-create.yaml\n",
    "ansible-playbook --fork=1 playbooks/matrix-user-create.yaml\n",
    "id smith trinity neo\n",
    "getent passwd smith trinity neo\n",
    "sudo loginctl enable-linger smith trinity neo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558ef77-5943-43d2-8c3f-6971490c825b",
   "metadata": {},
   "source": [
    "### `pam_slurm_adopt.so` - restricting user access to compute node\n",
    "To make sure user would not ssh directly to a compute node and start a job without submitting to slurm, it is a common practice to deny user access to if they have no job running on the compute node. This is archived by using the pam_access.so and pam_slurm_adopt.so together.  \n",
    "First you need to make sure pam_slurm_adopt.so pam module is installed. For RHEL/Rocky, package slurm-pam_slurm is needed. For Debian, package slurm-smd-libpam-slurm-adopt is needed. Then you need to modify 2 files: `/etc/security/access.conf` and `/etc/pam.d/sshd`  \n",
    "ref: [pam_slurm_adopt - Administrative Access Configuration](/doc/pam_slurm_adopt.html#admin_access)  \n",
    "`/etc/security/access.conf`:\n",
    "```\n",
    "...\n",
    "account sufficient pam_access.so\n",
    "account ...\n",
    "account ...\n",
    "-account required pam_slurm_adopt.so\n",
    "...\n",
    "```\n",
    "`-account required pam_slurm_adopt.so` denys any user without a running job from ssh into the host, this line should be added after all other \"account\" line. However, the consequence of adding this line alone is that not even administrator could login without a running job. To resolve this issue, pam_access.so is used, by putting `account sufficient pam_access.so` in the front of other \"account\" line, user/group allowed by this module will by pass pam_slurm_adopt.so. Let's treat all user of group lyoko as administrator and set them to bypass `pam_slurm_adopt.so`\n",
    "\n",
    "`/etc/security/access.conf`:\n",
    "```\n",
    "+:(lyoko):ALL\n",
    "-:ALL:ALL\n",
    "```\n",
    "First line allows member of group lyoko to login from anywhere, second line blocks everyone else, and they will go down the chain in pam. Now let's test it using your own user (group lyoko) and user smith (group matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315fa3c-8996-428e-97e8-4131c10e7172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Content of /etc/pam.d/sshd\n",
    "grep -vE \"^[#]\" /etc/pam.d/sshd\n",
    "\n",
    "# ensure this file is sent to compute node\n",
    "ansible -m copy -a \"src=/etc/pam.d/sshd dest=/etc/pam.d/sshd\" slurmd_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a87b0-f988-4a93-aa22-950f6d126bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Content of /etc/security/access.conf\n",
    "grep -vE \"^[#]\" /etc/security/access.conf\n",
    "\n",
    "# ensure this file is sent to compute node\n",
    "ansible -m copy -a \"src=/etc/security/access.conf dest=/etc/security/access.conf\" slurmd_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f6fff-6f43-46bd-923c-56518c050a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try ssh to a compute node as yourself and as smith, without a job\n",
    "set -x\n",
    "one_compute_node=$(scontrol show node --oneliner | awk '{print $1}' | cut -c10- | head -n1)\n",
    "echo ${one_compute_node}\n",
    "ssh ${one_compute_node} whoami\n",
    "sudo -i -u smith ssh ${one_compute_node} whoami\n",
    "set +x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfacca-094b-4118-a355-8205ba3c8e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit a job as smith\n",
    "set -x\n",
    "one_compute_node=$(scontrol show node --oneliner | awk '{print $1}' | cut -c10- | head -n1)\n",
    "# submit a job as smith\n",
    "smith_job_id=$(sudo -i -u smith sbatch -w ${one_compute_node} --parsable --output=/dev/null -D ~smith --time 00:10:00 ~smith/tutorials/helloworld.sh)\n",
    "\n",
    "# wait 10 sec for the job to start and then try ssh as smith\n",
    "sleep 10\n",
    "sudo squeue -j ${smith_job_id}\n",
    "sudo -i -u smith ssh ${one_compute_node} whoami\n",
    "\n",
    "# kill that job\n",
    "sudo scancel ${smith_job_id}\n",
    "\n",
    "# try login again after 30 sec\n",
    "sleep 30\n",
    "sudo -i -u smith ssh ${one_compute_node} whoami\n",
    "set +x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ec776-08e7-4688-ae49-64ad7a64463c",
   "metadata": {},
   "source": [
    "### `nss_slurm` - Providing user information by slurm  \n",
    "reference: [Name Service Caching Through NSS Slurm](/doc/nss_slurm.html)  \n",
    "`nss_slurm` is a name service caching that provides user information, including passwd and group information within a running. This is very usefull at scale, avoiding spikes of query overloading your LDAP/AD/NIS server when large scale parallel job starts, causing either your authentication service or your job to crash. However, this is not meant to replace your proper authentication service, only to reduce reliance and improve stability of the cluster as a whole.  \n",
    "To enable this feature, library libnss_slurm.so needs to be installed. For RHEL/Rocky it is included in the core slurm package already; for Debian, you need to install package `slurm-smd-libnss-slurm`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc62c31-0017-4032-a0cc-e17f494b7f09",
   "metadata": {},
   "source": [
    "**Master Node Configuration**  \n",
    "On the master node side, we need to make sure this feature is enabled. Check if LaunchParameters includes the `enable_nss_slurm` flag. Modify slurm.conf and restart slurmctld if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d1bea-4ef0-4c22-914b-b2bbde24bb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scontrol show config | grep LaunchParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4349f4-e718-4f50-b104-631a6082e5c7",
   "metadata": {},
   "source": [
    "**Compute Node Configuration**  \n",
    "On compute node, you need to add `slurm` as a source of passwd and group in `/etc/nsswitch.conf`. You should put it in front of other network source like sss and ldap, otherwise you would not be able to alleviate their load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c0a95-83ec-4f7f-a4b5-a1700626bb54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check /etc/nsswitch.conf on all compute node\n",
    "for compute in $(scontrol show node --oneliner | awk '{print $1}' | cut -c10-); do\n",
    "    echo ${compute}:\n",
    "    ssh ${compute} grep -E \"'^(passwd|group)'\" /etc/nsswitch.conf\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ea8d0-2f6b-469d-a4f3-3e5cf623fb2d",
   "metadata": {},
   "source": [
    "To test this setup, we will try to query the information of user `trinity` inside and outside of a running job. Note that user `trinity` is only created on this node (client) and master node only. User trinity only exist within a running job on the compute node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b703761-5aed-4d02-8a45-c87bf3a76336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set -x\n",
    "one_compute_node=$(scontrol show node --oneliner | awk '{print $1}' | cut -c10- | head -n1)\n",
    "ssh ${one_compute_node} id trinity\n",
    "sudo -i -u trinity srun -w ${one_compute_node} -D ~trinity id trinity\n",
    "set +x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726251d-4750-404c-b72f-fa9b8c1a2ebd",
   "metadata": {},
   "source": [
    "### `AuthType=auth/slurm` - un-authenticated master and compute, without munge\n",
    "reference: [Authentication Plugins - slurm](/doc/authentication.html#slurm)  \n",
    "Since slurm 23.11, auth/slurm, a new authentication plugin is introduced. Unlike munge, this new plugin allows the control plane (slurmctld, and slurmdbd) to function normally without the host being authenticated. Combining with nss_slurm, it is possible to run a slurm cluster with only the submittion/client node being authenticated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f41e81-fa3a-4213-94df-c135efa31f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example: user neo is not able to submit job if using munge\n",
    "if [[ $(scontrol show config | grep -i AuthType | awk '{print $3}' ) == auth/munge ]]; then\n",
    "    sudo -i -u neo srun whoami # this is supposed to fail\n",
    "else\n",
    "    echo Not using munge\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05016d3-d166-4d40-9266-6f87edeaf314",
   "metadata": {},
   "source": [
    "**Modify `slurm.conf` and `slurmdbd.conf`**  \n",
    "To select this auth plugin instead of munge, you need to set parameters in slurm.conf and slurmdbd.coonf as follow:  \n",
    "`slurm.conf`:\n",
    "```\n",
    "AuthType=auth/slurm\n",
    "CredType=cred/slurm\n",
    "AuthInfo=use_client_ids\n",
    "```  \n",
    "`slurmdbd.conf`:\n",
    "```\n",
    "AuthType=auth/slurm\n",
    "AuthInfo=use_client_ids\n",
    "```  \n",
    "A randomly generated pre-shared key /etc/slurm/slurm.key is also required:\n",
    "```bash\n",
    "dd if=/dev/random of=/etc/slurm/slurm.key bs=1024 count=1\n",
    "chown slurm:slurm /etc/slurm/slurm.key\n",
    "chmod 600 /etc/slurm/slurm.key\n",
    "```\n",
    "(This key has been generated during container image build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f732b-2f14-40c0-b7a2-79e9bdc69555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Switch to auth/slurm\n",
    "cat playbooks/use-slurm-auth.yaml\n",
    "ansible-playbook playbooks/use-slurm-auth.yaml\n",
    "scontrol show config | grep -i AuthType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43467b-749e-499f-ac89-c77e198758a1",
   "metadata": {},
   "source": [
    "Now we can try to submit a job as user neo again, note that this user only exist on this submission/client node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c4986-9772-4790-b34a-0ac1491cdfbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sudo -i -u neo srun bash -c \"whoami ; hostname ;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997b07c",
   "metadata": {},
   "source": [
    "For this slurm-lab container cluster, you can recreate the whole cluster to use auth/slurm by setting `AUTHTYPE=auth/slurm` in .env or pass as environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bee440-1123-4ef1-83f3-99d4760a67be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cleanup users\n",
    "sudo loginctl disable-linger smith trinity neo\n",
    "ansible-playbook --fork=1 playbooks/matrix-user-delete.yaml\n",
    "\n",
    "# (optional) switch back to munge\n",
    "#ansible-playbook playbooks/use-munge-auth.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064c2e6-7288-44ff-968b-1be13166fa28",
   "metadata": {},
   "source": [
    "## Saving & Retrieving Job Script and Environment variables\n",
    "Slurm can be configured to save the job script and running environment variables in the accounting database. This could be useful for debugging and investigation purpose.  \n",
    "To enable this feature you need to ensure `AccountingStoreFlags` includes `job_env` and `job_script` in `slurm.conf`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db365e67-2709-458c-a09e-5509caf9472e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scontrol show config | grep AccountingStoreFlags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e0414-8496-458d-8b86-a142894a60f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit a job via stdin, so the script cannot be found anywhere in the file system\n",
    "# jobid=$(sbatch --nodes=1 --ntasks-per-node=2 --parsable --time 00:20:00 --partition debug --begin ${start_time} endless-checksum-mpi.sh)\n",
    "jobid=$(\n",
    "sbatch --parsable --time 00:10:00 <<EOF\n",
    "#!/bin/bash\n",
    "echo \"I am a dummy job\"\n",
    "md5sum /dev/zero\n",
    "EOF\n",
    ")\n",
    "\n",
    "# show job status \n",
    "squeue -j ${jobid} -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce28b2-97e0-43f4-97c3-010004f1b670",
   "metadata": {},
   "source": [
    "To show job script, use `sacct` command with option `--batch-script`, it is required to specify the job id as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9438fee-43ee-48c2-a6b8-bbf984885ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show job script \n",
    "sacct -j ${jobid} --batch-script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863b249-7632-4706-9ed7-e6efd0a72de3",
   "metadata": {},
   "source": [
    "To list job environment variables, use `sacct` command with option `--env-vars`. This option is mutually exclusive with `--batch-script`, and it is required to specify the job id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb7fe3-0e87-4bf8-89db-f53508959b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show job environment \n",
    "sacct -j ${jobid} --env-vars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
