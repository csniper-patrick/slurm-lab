{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d3f131-d799-48a0-972a-6717836d98d7",
   "metadata": {},
   "source": [
    "# MPI Guide\n",
    "In this guide we will be running some multi node job using the message passing interface (MPI). Since Slurm is a popular scheduler used by many HPC and supercomputer, mainstream MPI implementation has built-in support for slurm. If you launch a MPI software within a slurm job, it is able to recognise the slurm environment and launch the software accordingly (ie. launching right amount of paralle process and use the correct allocated nodes). So you don't need to bother writing a machinefile/hostfile, or manually putting the `-np` option.    \n",
    "For more information check out [Slurm - MPI User Guide](/doc/mpi_guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fe6b5-e25c-45c2-9020-865a555fb08d",
   "metadata": {},
   "source": [
    "## Example: calculate $\\pi$\n",
    "In this example, we would estimate the value of $\\pi$ using the [Monte Carlo Method](https://en.wikipedia.org/wiki/Monte_Carlo_methodhttps://en.wikipedia.org/wiki/Monte_Carlo_method), with [OpenMPI](https://www.open-mpi.org/). \n",
    "In this lab environment, some of the software and libraries are managed using [Environment Modules](https://modules.readthedocs.io/en/latest/), which is a very convenient way of manage multiple libraries, software, different or even conflicting version of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd303e0-25e3-4577-b6b5-ad78c2edbb2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check available modules\n",
    "module avail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7967d1-72db-46fe-bd12-17e534321ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the mpi module\n",
    "module load mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02111e4a-1f54-49c2-b6ec-71d89e44d86a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list loaded modules\n",
    "module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c51cb-033a-4229-963f-529c5aa3f84b",
   "metadata": {},
   "source": [
    "Next we can take a look at the code and build it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a66bf-e9fa-4116-8261-1b75d28eb461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat mpi-pi/parallel-pi.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29d8fb-a9a9-48d2-9049-2873e999bde4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make --directory mpi-pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d132d-56a0-4ad2-bf41-b338f0f508fc",
   "metadata": {},
   "source": [
    "Now we are ready to run the code. Slurm provide many way of running a MPI program, one of them is using the `--mpi` option of srun. With this option you can launch the mpi program even from the submission host and see the stdout right there, but the actual execution happens in the compute node. For more detail of the option check out the manpage of [`srun --mpi`](/doc/srun.html#OPT_mpi).  \n",
    "For starters, use option `--ntasks <N>` to specify how many MPI process you would like to run. If you have more specific requirement of no. of nodes, process, memory, you could use a combination of `--nodes`, `--ntasks-per-node`, `--cpus-per-task`, `--mem` options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29961a-5e2e-451d-af2d-1562aa36eea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2 parallel process on 1 node\n",
    "srun --nodes=1 --ntasks-per-node=2 --mpi=pmi2 mpi-pi/parallel-pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ee01c-e21e-4a3e-be72-c60a11433e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8 parallel process, cross node\n",
    "srun --ntasks=8 --mpi=pmi2 mpi-pi/parallel-pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221261fb-0dfd-4e52-a025-2c1b857d72c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# request 4 nodes, 2 process on each node\n",
    "srun --nodes=4 --ntasks-per-node=2 --mem=0 --mpi=pmi2 mpi-pi/parallel-pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608768f-5908-4995-ad6a-796bac06201d",
   "metadata": {},
   "source": [
    "You might find it weird to see a multi node execution run much slower then single node run, That is because of MPI_Reduce being called unnecessary often. Each time this function is called, a barrier is setup, all process stop and synchronized to exchange data, and this is a very costly operation across node.  \n",
    "In the next section we will run the HPL benchmark, which doesn't have such an issue, and even offer OpenMP multithreading option to further reduce cross-node synchronization and communication. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d83eef-fa35-47ed-9f1f-4b29ad53e46c",
   "metadata": {},
   "source": [
    "## HPL Benchmark\n",
    "The [High Perfomance Linpack (HPL)](https://netlib.org/benchmark/hpl/) is a common benchmark in HPC/Supercomputing, It measures how many Floating-point operations per second (FLOPS) a cluster is capable of doing to rate it computational power. HPL is commonly use in ranking the best supercomputer in the world, UAT of new cluster/hardware, or as a stress test after hardware replacement in HPC. In this section we will build and run the HPL benchmark via slurm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d4bda-3902-4cda-8cee-95e8f6b2c431",
   "metadata": {},
   "source": [
    "### Install Spack\n",
    "[Spack](https://spack.io/) is a HPC software package manager, many compilers and HPC software are available and they are build from source locally when you install them. It is one of the 10 initial project in the [High Performance Software Foundation](https://hpsfoundation.github.io/#projects) formed by the [Linux Foundation](https://www.linuxfoundation.org/press/linux-foundation-announces-intent-to-form-high-performance-software-foundation-hpsf). We are going to install spack into our container lab cluster, and then build the HPL benchmark using spack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2105111-aa92-4177-ba68-1e818e85b813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "git clone -c feature.manyFiles=true https://github.com/spack/spack.git ~/.local/spack\n",
    "git -C ~/.local/spack checkout v0.22.3\n",
    "\n",
    "# add this line to setup spack on login\n",
    "ansible -m lineinfile -a \"path=${HOME}/.bashrc line='source ~/.local/spack/share/spack/setup-env.sh'\" localhost\n",
    "\n",
    "# activate spack\n",
    "source ~/.local/spack/share/spack/setup-env.sh\n",
    "which spack\n",
    "spack config add modules:default:enable:[lmod]\n",
    "\n",
    "# detect available compilers\n",
    "spack compiler find\n",
    "spack compilers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f72d4-1736-4358-8bb8-342d8bd04574",
   "metadata": {},
   "source": [
    "### Install HPL with Spack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed0791c-2676-4a09-8076-bcf7dce8dea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spack list hpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edadb7b-e256-41cb-a474-7a63089fb240",
   "metadata": {},
   "source": [
    "List and confirm the configuration spack is going to use for installing hpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d45946-4304-4330-aef5-0dbc89464122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spack spec hpl+openmp^openmpi+internal-pmix+internal-hwloc schedulers=slurm ^slurm+pmix\n",
    "spack spec hpl+openmp^openmpi+internal-pmix+internal-hwloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf3567f",
   "metadata": {},
   "source": [
    "Building HPL and all the dependency in parallel, span across 2 node. Make sure your spack is installed on a shared flock-supported file system, and you didn't turn off the default locking mechanism of spack. This could go terribly wrong otherwise. This installation takes long time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac1d65-7fdf-414d-8050-e7eec5d805b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# srun --nodes=4 --ntasks-per-node=1 --exclusive spack install hpl+openmp^openmpi+internal-pmix+internal-hwloc schedulers=slurm ^slurm+pmix\n",
    "srun --nodes=4 --ntasks-per-node=1 --exclusive spack install hpl+openmp^openmpi+internal-pmix+internal-hwloc\n",
    "\n",
    "# verify hpl has been installed & setup module\n",
    "spack find hpl\n",
    "\n",
    "for mod_path in $( find ~/.local/spack/share/spack/lmod -iname \"*.lua\" | xargs dirname | xargs dirname | uniq ); do\n",
    "    module use $mod_path\n",
    "    ansible -m lineinfile -a \"path=${HOME}/.bashrc line='module use $mod_path'\" localhost\n",
    "done\n",
    "\n",
    "module avail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae6e1a-f0a2-4a7a-b670-31022beb5f9a",
   "metadata": {},
   "source": [
    "### Run HPL\n",
    "To run the HPL benchmark we need to prepare a HPL.dat file that describe the problem size and the configuration of running the benchmark, we can also choose between running it in pure multi-process MPI or a hybrid MPI + OpenMP execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0bf17-d485-4ac6-a0fb-9b807d4723a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load hpl from module or spack\n",
    "module load hpl || spack load hpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0aace2-2181-4e8e-85d1-88a38b966d87",
   "metadata": {},
   "source": [
    "Example HPL.dat file. For detail and tuning of these parameters please refer to the [HPL Tuning Guide](https://www.netlib.org/benchmark/hpl/tuning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2692e-871c-4d32-bae5-3e849c758c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat ./HPL.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc4b08-e305-4be5-bb18-8654da857b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just MPI\n",
    "srun --ntasks=8 --pty mpirun --bind-to none xhpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8015a9-1b80-4e36-8954-20296ea45b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MPI + OpenMP hybrid run\n",
    "OMP_NUM_THREADS=2 srun --nodes=4 --ntasks-per-node=1 --cpus-per-task=2 --pty mpirun --bind-to none xhpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78370661-67e3-41fc-acf2-40d4cfa884be",
   "metadata": {},
   "source": [
    "Next is an example of sbatch HPL running script, that generates a HPL.dat file using environment variables provided by Slurm. This allows a bigger longer run when more resource is requested for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f6d7b-d94f-429f-b647-b0f988e6da22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example sbatch job script\n",
    "cat ./hpl-job.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51d466-c74f-4d83-9b56-54a4cbd2442e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MPI + OpenMP Hybrid run with sbatch job script\n",
    "sbatch --nodes=4 --ntasks-per-node=1 --cpus-per-task=2 ./hpl-job.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
