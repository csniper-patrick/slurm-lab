{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d3f131-d799-48a0-972a-6717836d98d7",
   "metadata": {},
   "source": [
    "# MPI Guide\n",
    "In this guide, we will be running some multi-node jobs using the Message Passing Interface (MPI). Since Slurm is a popular scheduler used by many HPCs and supercomputers, mainstream MPI implementations have built-in support for it. If you launch MPI software within a Slurm job, it is able to recognize the Slurm environment and launch the software accordingly (i.e., launching the right amount of parallel processes and using the correct allocated nodes). So you don't need to bother writing a machinefile/hostfile or manually putting in the `-np` option.    \n",
    "For more information check out [Slurm - MPI User Guide](/doc/mpi_guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fe6b5-e25c-45c2-9020-865a555fb08d",
   "metadata": {},
   "source": [
    "## Example: calculate $\\pi$\n",
    "In this example, we will estimate the value of $\\pi$ using the [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method) with [OpenMPI](https://www.open-mpi.org/). \n",
    "In this lab environment, some of the software and libraries are managed using [Environment Modules](https://modules.readthedocs.io/en/latest/), which is a very convenient way of managing multiple libraries, software, and different or even conflicting versions of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd303e0-25e3-4577-b6b5-ad78c2edbb2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check available modules\n",
    "module avail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7967d1-72db-46fe-bd12-17e534321ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the mpi module\n",
    "module load mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02111e4a-1f54-49c2-b6ec-71d89e44d86a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list loaded modules\n",
    "module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c51cb-033a-4229-963f-529c5aa3f84b",
   "metadata": {},
   "source": [
    "Next, we can take a look at the code and build it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a66bf-e9fa-4116-8261-1b75d28eb461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat mpi-pi/parallel-pi.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29d8fb-a9a9-48d2-9049-2873e999bde4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make --directory mpi-pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d132d-56a0-4ad2-bf41-b338f0f508fc",
   "metadata": {},
   "source": [
    "Now we are ready to run the code. Slurm provides many ways of running an MPI program. One of them is by using the `--mpi` option of `srun`. With this option, you can launch the MPI program even from the submission host and see the stdout right there, but the actual execution happens on the compute node. For more details on the option, check out the man page for [`srun --mpi`](/doc/srun.html#OPT_mpi).  \n",
    "For starters, use the option `--ntasks <N>` to specify how many MPI processes you would like to run. If you have more specific requirements for the number of nodes, processes, or memory, you could use a combination of the `--nodes`, `--ntasks-per-node`, `--cpus-per-task`, and `--mem` options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29961a-5e2e-451d-af2d-1562aa36eea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2 parallel process on 1 node\n",
    "srun --nodes=1 --ntasks-per-node=2 --mpi=pmix mpi-pi/parallel-pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ee01c-e21e-4a3e-be72-c60a11433e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8 parallel process, cross node\n",
    "srun --ntasks=8 --mpi=pmix mpi-pi/parallel-pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221261fb-0dfd-4e52-a025-2c1b857d72c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# request 4 nodes, 2 process on each node\n",
    "srun --nodes=4 --ntasks-per-node=2 --mem=0 --mpi=pmix mpi-pi/parallel-pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608768f-5908-4995-ad6a-796bac06201d",
   "metadata": {},
   "source": [
    "You might find it weird to see a multi-node execution run much slower than a single-node run. That is because `MPI_Reduce` is being called unnecessarily often. Each time this function is called, a barrier is set up, all processes stop and synchronize to exchange data, and this is a very costly operation across nodes.  \n",
    "In the next section, we will run the HPL benchmark, which doesn't have such an issue and even offers an OpenMP multithreading option to further reduce cross-node synchronization and communication. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d83eef-fa35-47ed-9f1f-4b29ad53e46c",
   "metadata": {},
   "source": [
    "## HPL Benchmark\n",
    "The [High-Performance Linpack (HPL)](https://netlib.org/benchmark/hpl/) is a common benchmark in HPC/Supercomputing. It measures how many floating-point operations per second (FLOPS) a cluster is capable of doing to rate its computational power. HPL is commonly used in ranking the best supercomputers in the world, for UAT of new clusters/hardware, or as a stress test after hardware replacement in an HPC environment. In this section, we will build and run the HPL benchmark via Slurm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d4bda-3902-4cda-8cee-95e8f6b2c431",
   "metadata": {},
   "source": [
    "### Install Spack\n",
    "[Spack](https://spack.io/) is an HPC software package manager. Many compilers and HPC software are available, and they are built from source locally when you install them. It is one of the 10 initial projects in the [High Performance Software Foundation](https://hpsfoundation.github.io/#projects), formed by the [Linux Foundation](https://www.linuxfoundation.org/press/linux-foundation-announces-intent-to-form-high-performance-software-foundation-hpsf). We are going to install Spack into our container lab cluster and then build the HPL benchmark using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2105111-aa92-4177-ba68-1e818e85b813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "git clone -c feature.manyFiles=true https://github.com/spack/spack.git ~/.local/spack\n",
    "git -C ~/.local/spack checkout v1.0.2\n",
    "\n",
    "# add this line to setup spack on login\n",
    "ansible -m lineinfile -a \"path=${HOME}/.bashrc line='source ~/.local/spack/share/spack/setup-env.sh'\" localhost\n",
    "\n",
    "# activate spack\n",
    "source ~/.local/spack/share/spack/setup-env.sh\n",
    "which spack\n",
    "spack config add modules:default:enable:[lmod]\n",
    "\n",
    "# detect available compilers\n",
    "spack compiler find\n",
    "spack compilers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f72d4-1736-4358-8bb8-342d8bd04574",
   "metadata": {},
   "source": [
    "### Install HPL with Spack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed0791c-2676-4a09-8076-bcf7dce8dea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spack list hpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edadb7b-e256-41cb-a474-7a63089fb240",
   "metadata": {},
   "source": [
    "List and confirm the configuration Spack is going to use for installing HPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d45946-4304-4330-aef5-0dbc89464122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spack spec hpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf3567f",
   "metadata": {},
   "source": [
    "Building HPL and all its dependencies in parallel, spanning across 4 nodes. Make sure your Spack is installed on a shared, flock-supported file system, and that you haven't turned off the default locking mechanism of Spack. This could go terribly wrong otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac1d65-7fdf-414d-8050-e7eec5d805b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "srun --nodes=4 --ntasks-per-node=1 --exclusive spack install hpl\n",
    "\n",
    "# verify hpl has been installed & setup module\n",
    "spack find hpl\n",
    "\n",
    "for mod_path in $( find ~/.local/spack/share/spack/lmod -iname \"*.lua\" | xargs dirname | xargs dirname | uniq ); do\n",
    "    module use $mod_path\n",
    "    ansible -m lineinfile -a \"path=${HOME}/.bashrc line='module use $mod_path'\" localhost\n",
    "done\n",
    "\n",
    "module avail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae6e1a-f0a2-4a7a-b670-31022beb5f9a",
   "metadata": {},
   "source": [
    "### Run HPL\n",
    "To run the HPL benchmark, we need to prepare an `HPL.dat` file that describes the problem size and the configuration for running the benchmark. We can also choose between running it in pure multi-process MPI or a hybrid MPI + OpenMP execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0bf17-d485-4ac6-a0fb-9b807d4723a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load hpl from module or spack\n",
    "module load hpl || spack load hpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0aace2-2181-4e8e-85d1-88a38b966d87",
   "metadata": {},
   "source": [
    "Example `HPL.dat` file. For details and tuning of these parameters, please refer to the [HPL Tuning Guide (https://www.netlib.org/benchmark/hpl/tuning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2692e-871c-4d32-bae5-3e849c758c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat ./HPL.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8015a9-1b80-4e36-8954-20296ea45b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MPI + OpenMP hybrid run\n",
    "OMP_NUM_THREADS=2 srun --nodes=4 --ntasks-per-node=1 --cpus-per-task=2 --mpi=pmix xhpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78370661-67e3-41fc-acf2-40d4cfa884be",
   "metadata": {},
   "source": [
    "Next is an example of an sbatch HPL running script that generates an `HPL.dat` file using environment variables provided by Slurm. This allows for a bigger, longer run when more resources are requested for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f6d7b-d94f-429f-b647-b0f988e6da22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example sbatch job script\n",
    "cat ./hpl-job.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51d466-c74f-4d83-9b56-54a4cbd2442e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MPI + OpenMP Hybrid run with sbatch job script\n",
    "sbatch --nodes=4 --ntasks-per-node=1 --cpus-per-task=2 ./hpl-job.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
